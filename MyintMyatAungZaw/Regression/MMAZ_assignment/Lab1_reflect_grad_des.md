# Reflection on Gradient Descent

With the selective program of 'radio' from the dataset, I have learned a lot about using of functions strategically, and constructing flow of gradient descent optimization.
I was truly enlightened as how the work is done by 'Chains of functions' instead of using scikit learn library so that took me more time than expected to grasp the flow of coding cells by cells.
In order to expand my horizon, I tweak only the parameter of 'alpha' which is learning rate, and get the idea of variations of steps depending upon the magnitude of learning rate.(eg. _0.05_ took _261 steps_ but took only _93 steps_ while the alpha is _0.03_).
