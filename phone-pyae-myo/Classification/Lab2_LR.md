### What’s the difference between polynomial/linear regression and logistic regression?
Explain when you would use each one and how their outputs are different.

I used OLS for linear and polynomial regression to compare with logistic regression because that is the standard way these models are fit: they predict a continuous number and choose coefficients that minimize the sum of squared errors. Since our task is binary (fraud vs not fraud), these regression models do not naturally output probabilities or class labels the way logistic regression does. To make a fair, side‑by‑side comparison on classification metrics like precision, recall, F1, and confusion matrices, I converted their continuous outputs into class labels by applying a simple threshold. A threshold of 0.5 is a natural starting point when the target is encoded as 0 and 1 because it treats values above the midpoint as class 1 and below as class 0. It mirrors the common practice in logistic regression of classifying a probability above 0.5 as positive. It’s not the only possible choice—one could pick a different cutoff based on ROC/PR curves or business costs but it gives a straightforward baseline for comparison.

Looking at the results, Linear Regression with a 0.5 threshold shows very high overall accuracy because the dataset is imbalanced, but it misses many actual fraud cases (recall about 0.56 on the test set). This means the model often scores true positives below 0.5. Its AUC of about 0.976 tells that the ranking is decent overall, but the chosen threshold is not favorable to recall. Polynomial Regression (degree 2) improves recall noticeably (about 0.82 on the test set) and has a stronger F1 there, which suggests the extra flexibility helps catch more positives at the 0.5 cutoff. However, its train AUC is extremely high (about 0.992) compared to its test AUC (about 0.965), which is a sign of mild overfitting. It separates the training data very well but doesn’t generalize quite as strongly as logistic regression typically does.

This contrast is exactly why logistic regression is preferred for binary classification. Logistic models directly estimate probabilities in [0, 1], optimize a loss tailored to classification (log‑loss), and their scores are naturally interpretable as probabilities. Linear and polynomial regression, by design, are for predicting continuous values, and when we force them into classification with a threshold, they can yield good‑looking accuracy in imbalanced settings while still missing many positives. Using OLS plus a 0.5 threshold here was simply a controlled way to put regression models on the same evaluation footing as logistic regression so that I can clearly analyse how these models' behaviors differ.